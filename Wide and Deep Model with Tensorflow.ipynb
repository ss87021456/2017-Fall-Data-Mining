{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import gc\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../input/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(data_path + 'train.csv')\n",
    "df_test = pd.read_csv(data_path + 'test.csv')\n",
    "df_songs = pd.read_csv(data_path + 'songs.csv')\n",
    "df_songs_extra = pd.read_csv(data_path + 'song_extra_info.csv')\n",
    "df_members = pd.read_csv(data_path + 'members.csv',\n",
    "                         parse_dates=['registration_init_time', 'expiration_date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill those `song_length` are nan to 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_songs['song_length'].fillna(20000, inplace=True)\n",
    "df_songs['song_length'] = df_songs['song_length'].astype(np.uint32)\n",
    "\n",
    "#df_songs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert `isrc` to `song_year`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isrc_to_year(isrc):\n",
    "    if type(isrc) == str:\n",
    "        if int(isrc[5:7]) > 17:\n",
    "            return 1900 + int(isrc[5:7])\n",
    "        else:\n",
    "            return 2000 + int(isrc[5:7])\n",
    "    else:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_songs_extra['song_year'] = df_songs_extra['isrc'].apply(isrc_to_year)\n",
    "df_songs_extra.drop(['isrc'], axis = 1, inplace = True)\n",
    "\n",
    "#df_songs_extra.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_members['membership_days'] = df_members['expiration_date'].subtract(df_members['registration_init_time']).dt.days.astype(int)\n",
    "\n",
    "df_members['registration_year'] = df_members['registration_init_time'].dt.year\n",
    "df_members['registration_month'] = df_members['registration_init_time'].dt.month\n",
    "df_members['registration_day'] = df_members['registration_init_time'].dt.day\n",
    "df_members = df_members.drop(['registration_init_time'], axis=1)\n",
    "\n",
    "df_members['expiration_year'] = df_members['expiration_date'].dt.year\n",
    "df_members['expiration_month'] = df_members['expiration_date'].dt.month\n",
    "df_members['expiration_day'] = df_members['expiration_date'].dt.day\n",
    "df_members = df_members.drop(['expiration_date'], axis=1)\n",
    "\n",
    "#df_members.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_songs = df_songs.merge(df_songs_extra, on='song_id', how='left')\n",
    "\n",
    "del df_songs_extra;\n",
    "gc.collect();\n",
    "\n",
    "#df_songs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.merge(df_members, on='msno', how='left')\n",
    "df_test = df_test.merge(df_members, on='msno', how='left')\n",
    "\n",
    "df_train = df_train.merge(df_songs, on='song_id', how='left')\n",
    "df_test = df_test.merge(df_songs, on='song_id', how='left')\n",
    "\n",
    "del df_members, df_songs;\n",
    "gc.collect();\n",
    "\n",
    "#df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 7377418 entries, 0 to 7377417\n",
      "Data columns (total 25 columns):\n",
      "msno                  object\n",
      "song_id               object\n",
      "source_system_tab     object\n",
      "source_screen_name    object\n",
      "source_type           object\n",
      "target                int64\n",
      "city                  int64\n",
      "bd                    int64\n",
      "gender                object\n",
      "registered_via        int64\n",
      "membership_days       int64\n",
      "registration_year     int64\n",
      "registration_month    int64\n",
      "registration_day      int64\n",
      "expiration_year       int64\n",
      "expiration_month      int64\n",
      "expiration_day        int64\n",
      "song_length           int64\n",
      "genre_ids             object\n",
      "artist_name           object\n",
      "composer              object\n",
      "lyricist              object\n",
      "language              int64\n",
      "name                  object\n",
      "song_year             int64\n",
      "dtypes: int64(14), object(11)\n",
      "memory usage: 1.4+ GB\n"
     ]
    }
   ],
   "source": [
    "df_train['song_length'].fillna(0, inplace=True)\n",
    "df_train['language'].fillna(0, inplace=True)\n",
    "df_train['song_year'].fillna(0, inplace=True)\n",
    "df_train['song_length'] = df_train['song_length'].astype(np.int64)\n",
    "df_train['language'] = df_train['language'].astype(np.int64)\n",
    "df_train['song_year'] = df_train['song_year'].astype(np.int64)\n",
    "\n",
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df_train.columns:\n",
    "    if df_train[col].isnull().any():\n",
    "        if df_train[col].dtype == object:\n",
    "            df_test[col].fillna('null',inplace=True)\n",
    "            df_train[col].fillna('null',inplace=True)\n",
    "        else:\n",
    "            df_train[col].fillna(0,inplace=True)\n",
    "            df_test[col].fillna(0,inplace=True)\n",
    "#df_train['source_system_tab'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 2556790 entries, 0 to 2556789\n",
      "Data columns (total 25 columns):\n",
      "id                    int64\n",
      "msno                  object\n",
      "song_id               object\n",
      "source_system_tab     object\n",
      "source_screen_name    object\n",
      "source_type           object\n",
      "city                  int64\n",
      "bd                    int64\n",
      "gender                object\n",
      "registered_via        int64\n",
      "membership_days       int64\n",
      "registration_year     int64\n",
      "registration_month    int64\n",
      "registration_day      int64\n",
      "expiration_year       int64\n",
      "expiration_month      int64\n",
      "expiration_day        int64\n",
      "song_length           float64\n",
      "genre_ids             object\n",
      "artist_name           object\n",
      "composer              object\n",
      "lyricist              object\n",
      "language              float64\n",
      "name                  object\n",
      "song_year             float64\n",
      "dtypes: float64(3), int64(11), object(11)\n",
      "memory usage: 507.2+ MB\n"
     ]
    }
   ],
   "source": [
    "df_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "msno                  False\n",
       "song_id               False\n",
       "source_system_tab     False\n",
       "source_screen_name    False\n",
       "source_type           False\n",
       "target                False\n",
       "city                  False\n",
       "bd                    False\n",
       "gender                False\n",
       "registered_via        False\n",
       "membership_days       False\n",
       "registration_year     False\n",
       "registration_month    False\n",
       "registration_day      False\n",
       "expiration_year       False\n",
       "expiration_month      False\n",
       "expiration_day        False\n",
       "song_length           False\n",
       "genre_ids             False\n",
       "artist_name           False\n",
       "composer              False\n",
       "lyricist              False\n",
       "language              False\n",
       "name                  False\n",
       "song_year             False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TensorFlow version 1.4.1\n"
     ]
    }
   ],
   "source": [
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "# Set to INFO for tracking training, default is WARN \n",
    "\n",
    "print(\"Using TensorFlow version %s\" % (tf.__version__))\n",
    "CATEGORICAL_COLUMNS = [\"msno\", \"song_id\", \"source_system_tab\", \"source_screen_name\", \n",
    "                       \"source_type\", \"gender\", \"genre_ids\", \"artist_name\",\n",
    "                       \"composer\", \"lyricist\", \"name\"]\n",
    "\n",
    "CONTINUOUS_COLUMNS = ['city','bd','registered_via','membership_days','registration_year','registration_month',\n",
    "                      'registration_day','expiration_year','expiration_month','expiration_day','song_length',\n",
    "                      'language','song_year']\n",
    "\n",
    "TARGET_COLUMN = 'target'\n",
    "df_train['ans'] = df_train['target'].values\n",
    "df_train = df_train.drop(['target'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input function configured\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 400\n",
    "temp_labels = df_train['ans']\n",
    "def generate_input_fn(df_train, num_epochs=None, shuffle=True, batch_size=BATCH_SIZE):\n",
    "    df_train['ans'] = df_train['ans'].apply(lambda x: x == 1).astype(int)\n",
    "    #del df_train['target']\n",
    "    \n",
    "    return tf.estimator.inputs.pandas_input_fn(\n",
    "            x = df_train,\n",
    "            y = df_train['ans'],\n",
    "            batch_size = batch_size,\n",
    "            num_epochs=num_epochs,\n",
    "            shuffle=shuffle)\n",
    "\n",
    "print('input function configured')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Feature Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparse Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we build the sparse columns.<br>\n",
    "Use sparse_column_with_keys() for columns that we know all possible values for.<br>\n",
    "Use sparse_column_with_hash_bucket() for columns that we want the the library to automatically map values for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical columns configured\n"
     ]
    }
   ],
   "source": [
    "# The layers module contains many utilities for creating feature columns.\n",
    "\n",
    "# Categorical base columns.\n",
    "gender = tf.feature_column.categorical_column_with_vocabulary_list(key=\"gender\", \n",
    "                                                                   vocabulary_list=[\"female\", \"male\"])\n",
    "#number of unique in df_train's categorical data\n",
    "########################\n",
    "#msno 30755            #\n",
    "#song_id 359966        #\n",
    "#source_system_tab 9   #\n",
    "#source_screen_name 20 #\n",
    "#source_type 12        #\n",
    "#gender 2              #\n",
    "#genre_ids 572         #\n",
    "#artist_name 40582     #\n",
    "#composer 76064        #\n",
    "#lyricist 33889        #\n",
    "#name 234111           #\n",
    "########################\n",
    "\n",
    "msno = tf.feature_column.categorical_column_with_hash_bucket(\n",
    "  \"msno\", hash_bucket_size=30755)\n",
    "song_id = tf.feature_column.categorical_column_with_hash_bucket(\n",
    "  \"song_id\", hash_bucket_size=359966)\n",
    "source_system_tab = tf.feature_column.categorical_column_with_hash_bucket(\n",
    "  \"source_system_tab\", hash_bucket_size=9)\n",
    "source_screen_name = tf.feature_column.categorical_column_with_hash_bucket(\n",
    "  \"source_screen_name\", hash_bucket_size=20)\n",
    "source_type = tf.feature_column.categorical_column_with_hash_bucket(\n",
    "  \"source_type\", hash_bucket_size=12)\n",
    "genre_ids = tf.feature_column.categorical_column_with_hash_bucket(\n",
    "  \"genre_ids\", hash_bucket_size=572)\n",
    "artist_name = tf.feature_column.categorical_column_with_hash_bucket(\n",
    "  \"artist_name\", hash_bucket_size=40582)\n",
    "composer = tf.feature_column.categorical_column_with_hash_bucket(\n",
    "  \"composer\", hash_bucket_size=76064)\n",
    "lyricist = tf.feature_column.categorical_column_with_hash_bucket(\n",
    "  \"lyricist\", hash_bucket_size=33889)\n",
    "name = tf.feature_column.categorical_column_with_hash_bucket(\n",
    "  \"name\", hash_bucket_size=234111)\n",
    "\n",
    "print('Categorical columns configured')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continuous Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, configure the real-valued columns using real_valued_column()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Continuous columns configured\n"
     ]
    }
   ],
   "source": [
    "# Continuous base columns.\n",
    "################################################################################################################\n",
    "#CONTINUOUS_COLUMNS = ['city','bd','registered_via','membership_days','registration_month','registration_year',#\n",
    "#                      'registration_day','expiration_year','expiration_month','expiration_day','song_length', #\n",
    "#                      'language','song_year']                                                                 #\n",
    "################################################################################################################\n",
    "city = tf.feature_column.numeric_column(\"city\")\n",
    "bd = tf.feature_column.numeric_column(\"bd\")\n",
    "registered_via = tf.feature_column.numeric_column(\"registered_via\")\n",
    "membership_days  = tf.feature_column.numeric_column(\"membership_days\")\n",
    "registration_year = tf.feature_column.numeric_column(\"registration_year\")\n",
    "registration_month = tf.feature_column.numeric_column(\"registration_month\")\n",
    "registration_day = tf.feature_column.numeric_column(\"registration_day\")\n",
    "expiration_year = tf.feature_column.numeric_column(\"expiration_year\")\n",
    "expiration_month = tf.feature_column.numeric_column(\"expiration_month\")\n",
    "expiration_day  = tf.feature_column.numeric_column(\"expiration_day\")\n",
    "song_length = tf.feature_column.numeric_column(\"song_length\")\n",
    "language  = tf.feature_column.numeric_column(\"language\")\n",
    "song_year = tf.feature_column.numeric_column(\"song_year\")\n",
    "\n",
    "print('Continuous columns configured')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformations complete\n"
     ]
    }
   ],
   "source": [
    "# Transformations.\n",
    "bd_buckets = tf.feature_column.bucketized_column(\n",
    "    bd, boundaries=[5, 18, 25, 30, 35, 40, 45, 50, 55, 60, 65 ])\n",
    "\n",
    "msno_song_id = tf.feature_column.crossed_column(\n",
    "    [\"msno\", \"song_id\"], hash_bucket_size=int(1e6))\n",
    "\n",
    "bd_msno_song_id = tf.feature_column.crossed_column(\n",
    "    [bd_buckets, \"msno\", \"song_id\"], hash_bucket_size=int(1e7))\n",
    "\n",
    "country_occupation = tf.feature_column.crossed_column(\n",
    "    [\"native_country\", \"occupation\"], hash_bucket_size=int(1e4))\n",
    "\n",
    "print('Transformations complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group feature columns into 2 objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The wide columns are the sparse, categorical columns that we specified, as well as our hashed, bucket, and feature crossed columns.<br><br>\n",
    "The deep columns are composed of embedded categorical columns along with the continuous real-valued columns. Column embeddings transform a sparse, categorical tensor into a low-dimensional and dense real-valued vector. The embedding values are also trained along with the rest of the model. For more information about embeddings, see the TensorFlow tutorial on Vector Representations Words, or Word Embedding on Wikipedia.<br><br>\n",
    "The higher the dimension of the embedding is, the more degrees of freedom the model will have to learn the representations of the features. We are starting with an 8-dimension embedding for simplicity, but later you can come back and increase the dimensionality if you wish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wide and deep columns configured\n"
     ]
    }
   ],
   "source": [
    "# Wide columns and deep columns.\n",
    "##CATEGORICAL_COLUMNS = [\"msno\", \"song_id\", \"source_system_tab\", \"source_screen_name\", \n",
    "##                       \"source_type\", \"gender\", \"genre_ids\", \"artist_name\",\n",
    "##                       \"composer\", \"lyricist\", \"name\"]\n",
    "\n",
    "## Crossed_COLUMNS\n",
    "## msno_song_id, bd_msno_song_id\n",
    "\n",
    "\n",
    "################################################################################################################\n",
    "#CONTINUOUS_COLUMNS = ['city','bd','registered_via','membership_days','registration_month','registration_year',#\n",
    "#                      'registration_day','expiration_year','expiration_month','expiration_day','song_length', #\n",
    "#                      'language','song_year']                                                                 #\n",
    "################################################################################################################\n",
    "wide_columns = [msno, song_id, source_system_tab,\n",
    "                 source_screen_name, source_type, gender,\n",
    "                 genre_ids, artist_name,\n",
    "                 composer, lyricist,\n",
    "                 name, msno_song_id,bd_msno_song_id]\n",
    "\n",
    "#wide_columns = [msno, song_id, source_system_tab,\n",
    "#      source_screen_name, source_type, gender,\n",
    "#      genre_ids,\n",
    "#      name, msno_song_id,bd_msno_song_id]\n",
    "\n",
    "deep_columns = [\n",
    "    # Multi-hot indicator columns for columns with fewer possibilities\n",
    "    tf.feature_column.indicator_column(source_system_tab),\n",
    "    tf.feature_column.indicator_column(source_screen_name),\n",
    "    tf.feature_column.indicator_column(source_type),\n",
    "    tf.feature_column.indicator_column(gender),\n",
    "    # Embeddings for categories with more possibilities\n",
    "    tf.feature_column.embedding_column(msno, dimension=100),\n",
    "    tf.feature_column.embedding_column(song_id, dimension=100),\n",
    "    tf.feature_column.embedding_column(genre_ids, dimension=8),\n",
    "    #tf.feature_column.embedding_column(artist_name, dimension=8),\n",
    "    #tf.feature_column.embedding_column(composer, dimension=8),\n",
    "    #tf.feature_column.embedding_column(lyricist, dimension=8),\n",
    "    tf.feature_column.embedding_column(name, dimension=8),\n",
    "    # Numerical columns\n",
    "    city,\n",
    "    bd,\n",
    "    registered_via,\n",
    "    membership_days,\n",
    "    registration_month,\n",
    "    registration_year,\n",
    "    registration_day,\n",
    "    expiration_year,\n",
    "    expiration_month,\n",
    "    expiration_day,\n",
    "    song_length,\n",
    "    language,\n",
    "    song_year\n",
    "]\n",
    "\n",
    "print('wide and deep columns configured')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model directory = models/model_WIDE_AND_DEEP\n",
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_task_type': 'worker', '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fbe451841d0>, '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 0, '_tf_random_seed': None, '_master': '', '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_model_dir': 'models/model_WIDE_AND_DEEP', '_save_summary_steps': 100}\n",
      "estimator built\n"
     ]
    }
   ],
   "source": [
    "def create_model_dir(model_type):\n",
    "    return 'models/model_' + model_type\n",
    "\n",
    "# If new_model=False, pass in the desired model_dir \n",
    "def get_model(model_type, new_model=False, model_dir=None):\n",
    "    if new_model or model_dir is None:\n",
    "        model_dir = create_model_dir(model_type) # Comment out this line to continue training a existing model\n",
    "    print(\"Model directory = %s\" % model_dir)\n",
    "    \n",
    "    m = None\n",
    "    \n",
    "    # Linear Classifier\n",
    "    if model_type == 'WIDE':\n",
    "        m = tf.estimator.LinearClassifier(\n",
    "            model_dir=model_dir, \n",
    "            feature_columns=wide_columns)\n",
    "\n",
    "    # Deep Neural Net Classifier\n",
    "    if model_type == 'DEEP':\n",
    "        m = tf.estimator.DNNClassifier(\n",
    "            model_dir=model_dir,\n",
    "            feature_columns=deep_columns,\n",
    "            hidden_units=[100, 50])\n",
    "\n",
    "    # Combined Linear and Deep Classifier\n",
    "    if model_type == 'WIDE_AND_DEEP':\n",
    "        m = tf.estimator.DNNLinearCombinedClassifier(\n",
    "                model_dir=model_dir,\n",
    "                linear_feature_columns=wide_columns,\n",
    "                dnn_feature_columns=deep_columns,\n",
    "                dnn_hidden_units=[128, 64, 32, 32])\n",
    "        \n",
    "    print('estimator built')\n",
    "    \n",
    "    return m, model_dir\n",
    "    \n",
    "MODEL_TYPE = 'WIDE_AND_DEEP'\n",
    "model_dir = create_model_dir(model_type=MODEL_TYPE)\n",
    "m, model_dir = get_model(model_type = MODEL_TYPE, model_dir=model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit the model (train it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from models/model_WIDE_AND_DEEP/model.ckpt-5000\n",
      "INFO:tensorflow:Saving checkpoints for 5001 into models/model_WIDE_AND_DEEP/model.ckpt.\n",
      "INFO:tensorflow:loss = 735.848, step = 5001\n",
      "INFO:tensorflow:global_step/sec: 68.9668\n",
      "INFO:tensorflow:loss = 1187.5, step = 5101 (1.450 sec)\n",
      "INFO:tensorflow:global_step/sec: 87.2534\n",
      "INFO:tensorflow:loss = 2120.15, step = 5201 (1.146 sec)\n",
      "INFO:tensorflow:global_step/sec: 85.9654\n",
      "INFO:tensorflow:loss = 6639.23, step = 5301 (1.163 sec)\n",
      "INFO:tensorflow:global_step/sec: 85.9558\n",
      "INFO:tensorflow:loss = 570.86, step = 5401 (1.163 sec)\n",
      "INFO:tensorflow:global_step/sec: 85.5207\n",
      "INFO:tensorflow:loss = 7585.72, step = 5501 (1.170 sec)\n",
      "INFO:tensorflow:global_step/sec: 85.7806\n",
      "INFO:tensorflow:loss = 686.797, step = 5601 (1.165 sec)\n",
      "INFO:tensorflow:global_step/sec: 86.7813\n",
      "INFO:tensorflow:loss = 1231.41, step = 5701 (1.152 sec)\n",
      "INFO:tensorflow:global_step/sec: 87.0735\n",
      "INFO:tensorflow:loss = 8147.14, step = 5801 (1.148 sec)\n",
      "INFO:tensorflow:global_step/sec: 86.3682\n",
      "INFO:tensorflow:loss = 3029.64, step = 5901 (1.158 sec)\n",
      "INFO:tensorflow:global_step/sec: 86.7429\n",
      "INFO:tensorflow:loss = 1739.38, step = 6001 (1.153 sec)\n",
      "INFO:tensorflow:global_step/sec: 86.2127\n",
      "INFO:tensorflow:loss = 3010.91, step = 6101 (1.160 sec)\n",
      "INFO:tensorflow:global_step/sec: 85.5076\n",
      "INFO:tensorflow:loss = 904.72, step = 6201 (1.169 sec)\n",
      "INFO:tensorflow:global_step/sec: 86.0678\n",
      "INFO:tensorflow:loss = 4382.33, step = 6301 (1.162 sec)\n",
      "INFO:tensorflow:global_step/sec: 86.114\n",
      "INFO:tensorflow:loss = 4249.18, step = 6401 (1.161 sec)\n",
      "INFO:tensorflow:global_step/sec: 86.046\n",
      "INFO:tensorflow:loss = 6135.61, step = 6501 (1.162 sec)\n",
      "INFO:tensorflow:global_step/sec: 84.7552\n",
      "INFO:tensorflow:loss = 4471.74, step = 6601 (1.180 sec)\n",
      "INFO:tensorflow:global_step/sec: 85.6354\n",
      "INFO:tensorflow:loss = 2041.97, step = 6701 (1.168 sec)\n",
      "INFO:tensorflow:global_step/sec: 86.4566\n",
      "INFO:tensorflow:loss = 3347.02, step = 6801 (1.156 sec)\n",
      "INFO:tensorflow:global_step/sec: 86.6275\n",
      "INFO:tensorflow:loss = 6527.16, step = 6901 (1.154 sec)\n",
      "INFO:tensorflow:global_step/sec: 86.8528\n",
      "INFO:tensorflow:loss = 7266.81, step = 7001 (1.151 sec)\n",
      "INFO:tensorflow:global_step/sec: 86.5745\n",
      "INFO:tensorflow:loss = 6468.4, step = 7101 (1.155 sec)\n",
      "INFO:tensorflow:global_step/sec: 86.0756\n",
      "INFO:tensorflow:loss = 536.663, step = 7201 (1.163 sec)\n",
      "INFO:tensorflow:global_step/sec: 85.1378\n",
      "INFO:tensorflow:loss = 1416.75, step = 7301 (1.174 sec)\n",
      "INFO:tensorflow:global_step/sec: 86.3644\n",
      "INFO:tensorflow:loss = 3791.0, step = 7401 (1.158 sec)\n",
      "INFO:tensorflow:global_step/sec: 87.5264\n",
      "INFO:tensorflow:loss = 6341.72, step = 7501 (1.142 sec)\n",
      "INFO:tensorflow:global_step/sec: 86.8347\n",
      "INFO:tensorflow:loss = 6201.7, step = 7601 (1.152 sec)\n",
      "INFO:tensorflow:global_step/sec: 86.0053\n",
      "INFO:tensorflow:loss = 6003.72, step = 7701 (1.163 sec)\n",
      "INFO:tensorflow:global_step/sec: 85.7584\n",
      "INFO:tensorflow:loss = 7493.94, step = 7801 (1.167 sec)\n",
      "INFO:tensorflow:global_step/sec: 87.4399\n",
      "INFO:tensorflow:loss = 1261.86, step = 7901 (1.143 sec)\n",
      "INFO:tensorflow:global_step/sec: 86.0341\n",
      "INFO:tensorflow:loss = 1113.05, step = 8001 (1.162 sec)\n",
      "INFO:tensorflow:global_step/sec: 85.8945\n",
      "INFO:tensorflow:loss = 553.952, step = 8101 (1.164 sec)\n",
      "INFO:tensorflow:global_step/sec: 85.7931\n",
      "INFO:tensorflow:loss = 6602.85, step = 8201 (1.166 sec)\n",
      "INFO:tensorflow:global_step/sec: 87.4125\n",
      "INFO:tensorflow:loss = 6379.24, step = 8301 (1.144 sec)\n",
      "INFO:tensorflow:global_step/sec: 86.3173\n",
      "INFO:tensorflow:loss = 1560.95, step = 8401 (1.159 sec)\n",
      "INFO:tensorflow:global_step/sec: 86.8411\n",
      "INFO:tensorflow:loss = 5031.37, step = 8501 (1.152 sec)\n",
      "INFO:tensorflow:global_step/sec: 86.6917\n",
      "INFO:tensorflow:loss = 4921.25, step = 8601 (1.153 sec)\n",
      "INFO:tensorflow:global_step/sec: 87.2379\n",
      "INFO:tensorflow:loss = 1703.06, step = 8701 (1.147 sec)\n",
      "INFO:tensorflow:global_step/sec: 86.5606\n",
      "INFO:tensorflow:loss = 1076.42, step = 8801 (1.155 sec)\n",
      "INFO:tensorflow:global_step/sec: 85.6725\n",
      "INFO:tensorflow:loss = 392.007, step = 8901 (1.167 sec)\n",
      "INFO:tensorflow:global_step/sec: 86.4854\n",
      "INFO:tensorflow:loss = 6322.64, step = 9001 (1.156 sec)\n",
      "INFO:tensorflow:global_step/sec: 86.1894\n",
      "INFO:tensorflow:loss = 6031.12, step = 9101 (1.160 sec)\n",
      "INFO:tensorflow:global_step/sec: 86.7815\n",
      "INFO:tensorflow:loss = 3810.6, step = 9201 (1.152 sec)\n",
      "INFO:tensorflow:global_step/sec: 86.5678\n",
      "INFO:tensorflow:loss = 4505.5, step = 9301 (1.155 sec)\n",
      "INFO:tensorflow:global_step/sec: 86.9499\n",
      "INFO:tensorflow:loss = 4374.99, step = 9401 (1.150 sec)\n",
      "INFO:tensorflow:global_step/sec: 86.8405\n",
      "INFO:tensorflow:loss = 5460.16, step = 9501 (1.152 sec)\n",
      "INFO:tensorflow:global_step/sec: 87.0554\n",
      "INFO:tensorflow:loss = 4387.71, step = 9601 (1.149 sec)\n",
      "INFO:tensorflow:global_step/sec: 86.3611\n",
      "INFO:tensorflow:loss = 5452.46, step = 9701 (1.158 sec)\n",
      "INFO:tensorflow:global_step/sec: 86.3615\n",
      "INFO:tensorflow:loss = 5026.54, step = 9801 (1.158 sec)\n",
      "INFO:tensorflow:global_step/sec: 87.0959\n",
      "INFO:tensorflow:loss = 1189.74, step = 9901 (1.148 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 10000 into models/model_WIDE_AND_DEEP/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 5971.18.\n",
      "training done\n",
      "CPU times: user 3min 18s, sys: 15.8 s, total: 3min 33s\n",
      "Wall time: 1min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "#train_file = str(\"adult.data.csv\") \n",
    "# \"gs://cloudml-public/census/data/adult.data.csv\"\n",
    "# storage.googleapis.com/cloudml-public/census/data/adult.data.csv\n",
    "#print(df_train)\n",
    "m.train(input_fn=generate_input_fn(df_train[:-1000]), \n",
    "        steps=5000)\n",
    "\n",
    "print('training done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Casting <dtype: 'float32'> labels to bool.\n",
      "WARNING:tensorflow:Casting <dtype: 'float32'> labels to bool.\n",
      "INFO:tensorflow:Starting evaluation at 2017-12-10-15:33:05\n",
      "INFO:tensorflow:Restoring parameters from models/model_WIDE_AND_DEEP/model.ckpt-10000\n",
      "INFO:tensorflow:Evaluation [1/50]\n",
      "INFO:tensorflow:Evaluation [2/50]\n",
      "INFO:tensorflow:Evaluation [3/50]\n",
      "INFO:tensorflow:Evaluation [4/50]\n",
      "INFO:tensorflow:Evaluation [5/50]\n",
      "INFO:tensorflow:Evaluation [6/50]\n",
      "INFO:tensorflow:Evaluation [7/50]\n",
      "INFO:tensorflow:Evaluation [8/50]\n",
      "INFO:tensorflow:Evaluation [9/50]\n",
      "INFO:tensorflow:Evaluation [10/50]\n",
      "INFO:tensorflow:Evaluation [11/50]\n",
      "INFO:tensorflow:Evaluation [12/50]\n",
      "INFO:tensorflow:Evaluation [13/50]\n",
      "INFO:tensorflow:Evaluation [14/50]\n",
      "INFO:tensorflow:Evaluation [15/50]\n",
      "INFO:tensorflow:Evaluation [16/50]\n",
      "INFO:tensorflow:Evaluation [17/50]\n",
      "INFO:tensorflow:Evaluation [18/50]\n",
      "INFO:tensorflow:Evaluation [19/50]\n",
      "INFO:tensorflow:Evaluation [20/50]\n",
      "INFO:tensorflow:Evaluation [21/50]\n",
      "INFO:tensorflow:Evaluation [22/50]\n",
      "INFO:tensorflow:Evaluation [23/50]\n",
      "INFO:tensorflow:Evaluation [24/50]\n",
      "INFO:tensorflow:Evaluation [25/50]\n",
      "INFO:tensorflow:Evaluation [26/50]\n",
      "INFO:tensorflow:Evaluation [27/50]\n",
      "INFO:tensorflow:Evaluation [28/50]\n",
      "INFO:tensorflow:Evaluation [29/50]\n",
      "INFO:tensorflow:Evaluation [30/50]\n",
      "INFO:tensorflow:Evaluation [31/50]\n",
      "INFO:tensorflow:Evaluation [32/50]\n",
      "INFO:tensorflow:Evaluation [33/50]\n",
      "INFO:tensorflow:Evaluation [34/50]\n",
      "INFO:tensorflow:Evaluation [35/50]\n",
      "INFO:tensorflow:Evaluation [36/50]\n",
      "INFO:tensorflow:Evaluation [37/50]\n",
      "INFO:tensorflow:Evaluation [38/50]\n",
      "INFO:tensorflow:Evaluation [39/50]\n",
      "INFO:tensorflow:Evaluation [40/50]\n",
      "INFO:tensorflow:Evaluation [41/50]\n",
      "INFO:tensorflow:Evaluation [42/50]\n",
      "INFO:tensorflow:Evaluation [43/50]\n",
      "INFO:tensorflow:Evaluation [44/50]\n",
      "INFO:tensorflow:Evaluation [45/50]\n",
      "INFO:tensorflow:Evaluation [46/50]\n",
      "INFO:tensorflow:Evaluation [47/50]\n",
      "INFO:tensorflow:Evaluation [48/50]\n",
      "INFO:tensorflow:Evaluation [49/50]\n",
      "INFO:tensorflow:Evaluation [50/50]\n",
      "INFO:tensorflow:Finished evaluation at 2017-12-10-15:33:06\n",
      "INFO:tensorflow:Saving dict for global step 10000: accuracy = 0.5733, accuracy_baseline = 0.5822, auc = 0.555987, auc_precision_recall = 0.453056, average_loss = 1.72535, global_step = 10000, label/mean = 0.4178, loss = 690.142, prediction/mean = 0.152731\n",
      "evaluate done\n",
      "Accuracy: 0.5733\n",
      "{'loss': 690.14154, 'accuracy_baseline': 0.58219999, 'global_step': 10000, 'auc': 0.55598724, 'prediction/mean': 0.15273054, 'label/mean': 0.41780001, 'average_loss': 1.725354, 'auc_precision_recall': 0.45305589, 'accuracy': 0.5733}\n"
     ]
    }
   ],
   "source": [
    "### test with last 1000 train_data\n",
    "results = m.evaluate(input_fn=generate_input_fn(df_train), \n",
    "                     steps=50)\n",
    "print('evaluate done')\n",
    "\n",
    "print('Accuracy: %s' % results['accuracy'])\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "id = df_test['id'].values\n",
    "#del df_test['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from models/model_WIDE_AND_DEEP/model.ckpt-10000\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "predict_input_fn = tf.estimator.inputs.pandas_input_fn(\n",
    "        x=df_test,\n",
    "        batch_size=1,\n",
    "        num_epochs=1,\n",
    "        shuffle=False)\n",
    "#  def predict(self, x=None, input_fn=None, batch_size=None, outputs=None,\n",
    "#              as_iterable=True):\n",
    "predictions = m.predict(input_fn=predict_input_fn)\n",
    "container = list(predictions)\n",
    "#for prediction in predictions[:100]:\n",
    "#    print(prediction['probabilities'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from models/model_WIDE_AND_DEEP/model.ckpt-5000\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "100000\n",
      "110000\n",
      "120000\n",
      "130000\n",
      "140000\n",
      "150000\n",
      "160000\n",
      "170000\n",
      "180000\n",
      "190000\n",
      "200000\n",
      "210000\n",
      "220000\n",
      "230000\n",
      "240000\n",
      "250000\n",
      "260000\n",
      "270000\n",
      "280000\n",
      "290000\n",
      "300000\n",
      "310000\n",
      "320000\n",
      "330000\n",
      "340000\n",
      "350000\n",
      "360000\n",
      "370000\n",
      "380000\n",
      "390000\n",
      "400000\n",
      "410000\n",
      "420000\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "count = 0\n",
    "for prediction in predictions:\n",
    "    count += 1\n",
    "    container.append(prediction['probabilities'][1])\n",
    "    if count % 10000 == 0 :\n",
    "        print(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save to submission.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur_time = datetime.datetime.now().strftime('%Y%m%d-%H%M%S')\n",
    "file_name = 'submission_' + cur_time + '.csv'\n",
    "\n",
    "subm = pd.DataFrame()\n",
    "subm['id'] = ids\n",
    "subm['target'] = p_test_1\n",
    "\n",
    "subm.to_csv(data_path + file_name, index=False, float_format = '%.5f')\n",
    "print('saved as ' + data_path + file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
